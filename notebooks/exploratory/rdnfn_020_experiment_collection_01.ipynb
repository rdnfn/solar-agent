{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cd09ba-0caf-460c-8dc2-112ef6198cac",
   "metadata": {},
   "source": [
    "# Experiment Collection #01\n",
    "\n",
    "This notebook contains experiments regarding the use of a penalty term and enabling charging from the grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112286d9-6b0d-45b1-8d69-e516c290c04b",
   "metadata": {},
   "source": [
    "## 1. Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5771fe1-6231-46b2-b4ce-98f9ce02e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e24471a-fa05-470f-9b50-b70c66c58953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77e9a3-1690-45ff-b5bf-8eef57339043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib\n",
    "import ray.tune \n",
    "import solara.envs.creator\n",
    "\n",
    "## Initialising ray (starts background process for distributed computing)\n",
    "ray.shutdown()\n",
    "ray.init(logging_level=\"WARNING\", object_store_memory= 25 * 10**9)\n",
    "\n",
    "# Adding environment creator function to ray\n",
    "ray.tune.registry.register_env(\"battery_control\", solara.envs.creator.create_env)\n",
    "\n",
    "# Output format of figures\n",
    "OUT_FORMAT = \".svg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbd3da-5d40-4fe2-b7f2-d018a85a4ae1",
   "metadata": {},
   "source": [
    "## 2. Experiment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6cdce-2d29-4fc9-94ac-d444dba8c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solara.constants import PROJECT_PATH\n",
    "import solara.utils.rllib\n",
    "\n",
    "EXPERIMENT_NAME = \"experiment_01_penalty_grid_ausgrid\"\n",
    "\n",
    "# RL environment configuration\n",
    "ENV_CONFIG = {\n",
    "    'general': {\n",
    "        'type': 'battery_control.BatteryControlEnv',\n",
    "        'infeasible_control_penalty': ray.tune.grid_search([False, True]),\n",
    "        'grid_charging': ray.tune.grid_search([True, False]),\n",
    "        'logging_level': \"RAY\", # if using RLlib, set to 'RAY'\n",
    "    },\n",
    "    'components': {\n",
    "        'battery': {\n",
    "            'type': 'LithiumIonBattery',\n",
    "            'size': 10,\n",
    "            'chemistry': 'NMC',\n",
    "            'time_step_len': 1,\n",
    "        },\n",
    "        'solar': {\n",
    "            'type': 'DataPV',\n",
    "            'data_path': PROJECT_PATH + \"/data/ausgrid/processed/house2_solar_gen.txt\",\n",
    "            'fixed_sample_num': 200,\n",
    "        },\n",
    "        'load': {\n",
    "            'type': 'DataLoad',\n",
    "            'data_path': PROJECT_PATH + \"/data/ausgrid/processed/house2_combined_load.txt\",\n",
    "            'fixed_sample_num': 200,\n",
    "        },\n",
    "        'grid': {\n",
    "            'type': 'PeakGrid',\n",
    "            'peak_threshold': 1.0,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# RL agent configuration\n",
    "AGENT_CONFIG = {\n",
    "    \"env\": \"battery_control\",\n",
    "    \"env_config\": ENV_CONFIG,\n",
    "    \"gamma\": 0.9999999,\n",
    "    \"lr\": 5e-5,\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [256, 256, 256, 256],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "        \"post_fcnet_activation\": \"tanh\",\n",
    "    },\n",
    "    # Utilities settings\n",
    "    \"framework\": \"torch\",\n",
    "    \"log_level\": \"WARNING\",\n",
    "    #\"num_workers\": 9,\n",
    "    #\"num_gpus\": 1,\n",
    "    \"callbacks\": solara.utils.rllib.InfoCallback,\n",
    "    \"seed\" : ray.tune.randint(0, 10000000),\n",
    "}\n",
    "\n",
    "# Full experiment configuration including RL algorithm type\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"run_or_experiment\": \"PPO\",\n",
    "    \"config\": AGENT_CONFIG,\n",
    "    \"stop\": {\"training_iteration\": 200},\n",
    "    \"name\": EXPERIMENT_NAME,\n",
    "    \"local_dir\": \"./tmp/tune/\",\n",
    "    \"log_to_file\": True,\n",
    "    \"checkpoint_freq\": 1,\n",
    "}\n",
    "\n",
    "# Other settings\n",
    "PLOT_DIR = PROJECT_PATH + \"/figures/experiments/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29c1e7-194b-4378-97c6-674a8da2afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelisation Setup\n",
    "if False:\n",
    "    num_workers = 4\n",
    "    gpu_count = 1\n",
    "    reserved_capacity = 0.01 # Driver GPU\n",
    "    num_gpus_per_worker = (gpu_count - reserved_capacity) / num_workers\n",
    "\n",
    "\n",
    "    AGENT_CONFIG[\"num_workers\"] = num_workers\n",
    "    AGENT_CONFIG[\"num_gpus\"] = num_gpus\n",
    "    AGENT_CONFIG[\"num_envs_per_worker\"]= 8\n",
    "    \n",
    "\n",
    "#AGENT_CONFIG[\"num_gpus\"] = 1\n",
    "#AGENT_CONFIG[\"num_envs_per_worker\"]= 8\n",
    "AGENT_CONFIG[\"num_workers\"] = 10\n",
    "AGENT_CONFIG[\"num_gpus\"] = 1\n",
    "#AGENT_CONFIG[\"remote_worker_envs\"]= True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f3d6c-bd3c-4fbb-bb41-76403744886a",
   "metadata": {},
   "source": [
    "## 3. Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa610149-a6e9-45c7-a4d9-0e422893d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting visualisation in notebook\n",
    "reporter = ray.tune.JupyterNotebookReporter(overwrite=True)\n",
    "reporter.add_metric_column(\"custom_metrics/cost_mean\")\n",
    "reporter.add_metric_column(\"custom_metrics/power_diff_mean\")\n",
    "\n",
    "# Running experiment\n",
    "analysis = ray.tune.run(\n",
    "    progress_reporter=reporter,\n",
    "    **EXPERIMENT_CONFIG,\n",
    "    #resume=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3899a-d98f-43f5-adb4-a840e25814d5",
   "metadata": {},
   "source": [
    "## 4. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7cace4-af1f-4d15-b5c4-7ff4e5468fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import solara.plot.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exp_path = EXPERIMENT_CONFIG[\"local_dir\"] + EXPERIMENT_CONFIG[\"name\"] + \"/\"\n",
    "#exp_path = \"./tmp/tune/PPO/\"\n",
    "state_files = [filename for filename in os.listdir(exp_path) if \"experiment_state\" in filename ]\n",
    "last_state_file = sorted(state_files, reverse=True)[0]\n",
    "\n",
    "analysis = ray.tune.ExperimentAnalysis(experiment_checkpoint_path=exp_path + last_state_file)\n",
    "trials = analysis.fetch_trial_dataframes()\n",
    "trials = {key: trials[key] for key in sorted(trials.keys())}  # Sort trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20346-a508-45db-82ba-b0ee92ec8d15",
   "metadata": {},
   "source": [
    "# 4.1 Training Progress Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7017eb-444a-4118-ab30-8ef36fab0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating helper function for plotting\n",
    "import numpy as np\n",
    "\n",
    "def plot_trials(trials, \n",
    "                necessary_cond=None,\n",
    "                other_conditions=None, \n",
    "                experiment_name=\"default_experiment\",\n",
    "                plot_name = \"plot_00_default\",\n",
    "                plot_dir = \"./figures\"):\n",
    "    \"\"\"Plot progress over iterations for experiments.\"\"\"\n",
    "    \n",
    "    solara.plot.pyplot.default_setup()\n",
    "    \n",
    "    other_conditions = list(other_conditions)\n",
    "    \n",
    "    for trial_name, trial_data in trials.items():\n",
    "        if necessary_cond is None or necessary_cond[1] in trial_name:\n",
    "            label = \"\"\n",
    "            \n",
    "            for i, (cond_label, condition) in enumerate(other_conditions):\n",
    "                if condition in trial_name:\n",
    "                    label += cond_label\n",
    "                else: \n",
    "                    label += \"no \" + cond_label\n",
    "                if i < len(other_conditions) - 1:\n",
    "                    label += \", \"\n",
    "            \n",
    "            label = label.capitalize()\n",
    "            \n",
    "            trace_len = 200\n",
    "            x_values = np.arange(1,trace_len+1)\n",
    "            ticks_gap = 25\n",
    "            x_ticks = [1] + list(np.arange(ticks_gap,150+1, ticks_gap)) + [trace_len]\n",
    "            trace = trial_data[\"custom_metrics/cost_mean\"][0:trace_len]\n",
    "            plt.plot(x_values,trace, label=label)\n",
    "            \n",
    "    plt.semilogy()\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Training iteration\")\n",
    "    plt.ylabel(\"Average cost per episode (\\$)\")\n",
    "    plt.xticks(x_ticks)\n",
    "    \n",
    "    plt.savefig(fname=plot_dir + experiment_name + \"_\" + plot_name + OUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a1097-fda7-48ab-ad3f-2e3a8762b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(trials, \n",
    "            necessary_cond=[\"grid charging\", \"grid_charging=False\"], \n",
    "            other_conditions=[[\"penalty\",\"infeasible_control_penalty=True\"]],\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            plot_dir=PLOT_DIR,\n",
    "            plot_name=\"plot_01_penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7140e0-8e24-4150-949a-69c72ec2a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(trials, \n",
    "            necessary_cond=[\"penalty\",\"infeasible_control_penalty=False\"], \n",
    "            other_conditions=[[\"grid charging\", \"grid_charging=True\"]],\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            plot_dir=PLOT_DIR,\n",
    "            plot_name=\"plot_02_grid_charging_no_penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5876108-f65f-40f0-9c43-e5e163189d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(trials, \n",
    "            necessary_cond=[\"penalty\",\"infeasible_control_penalty=True\"], \n",
    "            other_conditions=[[\"grid charging\", \"grid_charging=True\"]],\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            plot_dir=PLOT_DIR,\n",
    "            plot_name=\"plot_03_grid_charging_with_penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267a872-5d26-4e43-beeb-270568b1b6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_conditions = [[\"penalty\",\"infeasible_control_penalty=True\"],\n",
    "                    [\"grid charging\", \"grid_charging=True\"],]\n",
    "\n",
    "\n",
    "plot_trials(trials, necessary_cond=None, other_conditions=other_conditions,\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            plot_dir=PLOT_DIR,\n",
    "            plot_name=\"plot_04_all_trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467530b2-741c-4ae1-8032-b6ca188aaa7f",
   "metadata": {},
   "source": [
    "# 4.2 Policy Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62e43b-c3b6-47e4-954a-96a054cac3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def get_episode_data_from_checkpoint(exp_path: str, iteration_num: int):\n",
    "    \"\"\"Get episode data from loading policy from certain iteration of experiment.\"\"\"\n",
    "    \n",
    "    trial_agent_config = analysis.get_all_configs()[exp_path]\n",
    "\n",
    "    # Remove some unnecessary configs that may stop re-loading\n",
    "    trial_agent_config.pop(\"callbacks\")\n",
    "    trial_agent_config.pop(\"num_gpus\")\n",
    "    agent = ray.rllib.agents.ppo.PPOTrainer(config=trial_agent_config)\n",
    "\n",
    "    check_range=iteration_num\n",
    "    episodes_data = solara.utils.rllib.run_episodes_from_checkpoints(agent=agent, \n",
    "                                                                     check_save_path=exp_path, \n",
    "                                                                     check_range=check_range)\n",
    "    \n",
    "    if len(episodes_data) == 1:  \n",
    "        return episodes_data[0]\n",
    "    else:\n",
    "        return episodes_data\n",
    "\n",
    "def get_experiment_path(trials, grid_charging=True,penalty=True):\n",
    "    \"\"\"Get experiment paths\"\"\"\n",
    "    exp_path = [trial_path for trial_path in trials.keys() \n",
    "     if \"grid_charging={}\".format(grid_charging) in trial_path and \n",
    "     \"infeasible_control_penalty={}\".format(penalty) in trial_path][0]\n",
    "    return exp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c65b63-4ab4-4fab-b5d0-6873824513c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting configuration\n",
    "\n",
    "## Lines to draw in policy plot\n",
    "POLICY_PLOT_CONF = {\n",
    "    \"selected_keys\": ['load','pv_gen','energy_cont','net_load',\n",
    "              'charging_power','cost','price_threshold',\n",
    "              'actions'],\n",
    "    \"y_min\":-1.3,\n",
    "    \"y_max\":1.4,\n",
    "    \"show_grid\":False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3e7a3-1bbd-429e-8217-3bc2dc506f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "\n",
    "#matplotlib.use(\"pgf\")\n",
    "#matplotlib.rcParams.update({\n",
    "#    \"pgf.texsystem\": \"pdflatex\",\n",
    "#    'font.family': 'serif',\n",
    "#    'text.usetex': True,\n",
    "#    'pgf.rcfonts': False,\n",
    "#})\n",
    "\n",
    "exp_path = get_experiment_path(trials, grid_charging=False, penalty=False)\n",
    "episode_data = get_episode_data_from_checkpoint(exp_path, iteration_num=150)\n",
    "\n",
    "solara.plot.pyplot.plot_episode(episode_data,title=None, **POLICY_PLOT_CONF)\n",
    "plt.savefig(fname=PLOT_DIR + EXPERIMENT_NAME + \"_plot_05_policy_iter150_no_grid_no_penalty_failure\" + OUT_FORMAT, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3242fa5-c667-4aa2-af0a-9e931f8c5b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = get_experiment_path(trials, grid_charging=False, penalty=True)\n",
    "episode_data = get_episode_data_from_checkpoint(exp_path, iteration_num=150)\n",
    "\n",
    "solara.plot.pyplot.plot_episode(episode_data,title=None, **POLICY_PLOT_CONF)\n",
    "plt.savefig(fname=PLOT_DIR + EXPERIMENT_NAME + \"_plot_06_policy_iter150_no_grid_with_penalty\" + OUT_FORMAT, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a989e-9551-4aaf-aed1-f4e6b7184ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = get_experiment_path(trials, grid_charging=True, penalty=False)\n",
    "episode_data = get_episode_data_from_checkpoint(exp_path, iteration_num=150)\n",
    "\n",
    "solara.plot.pyplot.plot_episode(episode_data,title=None, **POLICY_PLOT_CONF)\n",
    "plt.savefig(fname=PLOT_DIR + EXPERIMENT_NAME + \"_plot_07_policy_iter150_grid_no_penalty_failure\" + OUT_FORMAT, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f993d3-f6ae-40c9-abe3-efce18c0bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = get_experiment_path(trials, grid_charging=True, penalty=True)\n",
    "episode_data = get_episode_data_from_checkpoint(exp_path, iteration_num=150)\n",
    "\n",
    "solara.plot.pyplot.plot_episode(episode_data,title=None, **POLICY_PLOT_CONF)\n",
    "plt.savefig(fname=PLOT_DIR + EXPERIMENT_NAME + \"_plot_08_policy_iter150_grid_with_penalty\" + OUT_FORMAT, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d12f3-e0cc-4fee-bc6a-6a34c53b8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widget\n",
    "\n",
    "exp_path = get_experiment_path(trials, grid_charging=True, penalty=True)\n",
    "episode_data = get_episode_data_from_checkpoint(exp_path, iteration_num=[1,151])\n",
    "\n",
    "import solara.plot.widgets\n",
    "solara.plot.widgets.InteractiveEpisodes(episode_data, \n",
    "                                        initial_visibility=POLICY_PLOT_CONF[\"selected_keys\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac8e73-88ec-4028-96a1-df7a4d29919a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
