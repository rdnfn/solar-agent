{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ec3f68-c82d-4793-b576-a28f7bce248c",
   "metadata": {},
   "source": [
    "# Introduction to Solar Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e143ba-af4e-4807-91da-7b3e4ba88e23",
   "metadata": {},
   "source": [
    "## 1. What problem does Solar Agent tackle?\n",
    "\n",
    "Solar Agent considers the battery control problem occuring in residential solar installation. Most residential solar installations have the following four parts:\n",
    "- **Solar panels:** a photovoltaic installation, for example on the roof.\n",
    "- **Battery:** a battery that is able to store energy from the solar installation or grid.\n",
    "- **Load:** the load that is used by the residents of the home.\n",
    "- **Grid:** the connection to the grid, with a certain pricing scheme for buying energy, and potentially selling energy.\n",
    "\n",
    "Their power, data and control connections to each other are shown in the diagram below.\n",
    "\n",
    "\n",
    "![img_001_environment_setup.png](../../docs/img/img_001_environment_setup.png)\n",
    "\n",
    "The goal is to minimise the cost incurred from using energy from the grid for home use. This goal is achieved by optimising the charging/discharging control of the battery. The Solar Agent project aims to create RL agents that are able to solve this control problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453f28d-7436-4e3f-8f85-19d1620ed85d",
   "metadata": {},
   "source": [
    "## 2. RL Problem Definition\n",
    "\n",
    "In order to apply reinforcement learning (RL), we rephrase the problem as an agent interacting with an environment, as below.\n",
    "\n",
    "![img_002_rl_setup.png](../../docs/img/img_002_rl_setup.png)\n",
    "\n",
    "In using the Solar Agent package, we can create such an environment using the function below. This creates an environment representing the simplest case of the battery control problem: solar generation and load are non-stochastic and always follow the same pattern, the grid uses peak-demand pricing (one price below and other above certain power draw threshold) and does not accept feed-in power into the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91869e8-7765-442a-a42f-1e22c2926e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter setup\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ed0aa-1799-4663-9eb8-0fdaf53b5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import solara.envs.components.solar\n",
    "import solara.envs.components.load\n",
    "import solara.envs.components.grid\n",
    "import solara.envs.components.battery\n",
    "import solara.envs.battery_control\n",
    "import solara.utils.logging\n",
    "from solara.constants import PROJECT_PATH\n",
    "\n",
    "def battery_env_creator(env_config=None):\n",
    "    \"\"\"Create a battery control environment.\"\"\"\n",
    "    \n",
    "    PV_DATA_PATH = PROJECT_PATH + \"/data/solar_trace_data/PV_5796.txt\"\n",
    "    LOAD_DATA_PATH = PROJECT_PATH + \"/data/solar_trace_data/load_5796.txt\"\n",
    "\n",
    "    # Setting up components of environment\n",
    "    battery_model = solara.envs.components.battery.LithiumIonBattery(size=10, \n",
    "                                                                     chemistry=\"NMC\", \n",
    "                                                                     time_step_len=1)\n",
    "    pv_model = solara.envs.components.solar.DataPV(data_path=PV_DATA_PATH,\n",
    "                                                   fixed_sample_num=12)\n",
    "    load_model = solara.envs.components.load.DataLoad(data_path=LOAD_DATA_PATH,\n",
    "                                                      fixed_sample_num=12)\n",
    "    grid_model = solara.envs.components.grid.PeakGrid(peak_threshold=1.0)\n",
    "\n",
    "    # Fixing load and PV trace to single sample\n",
    "    episode_num = 12\n",
    "    load_model.fix_start(episode_num)\n",
    "    pv_model.fix_start(episode_num)\n",
    "\n",
    "    env = solara.envs.battery_control.BatteryControlEnv(\n",
    "        battery = battery_model,\n",
    "        pv_system = pv_model,\n",
    "        grid = grid_model,\n",
    "        load = load_model,\n",
    "        infeasible_control_penalty=True,\n",
    "        grid_charging=True,\n",
    "        logging_level = \"RAY\",\n",
    "    )\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc8f61-9d4a-4835-b740-218d681b8196",
   "metadata": {},
   "source": [
    "## 3. Training an Agent\n",
    "In this section we train an agent using `RLlib`. \n",
    "\n",
    "\n",
    "### 3.1 Setup & Defining Agent\n",
    "We first setup an agent based proximal policy optimisation (PPO):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc12a0-1594-4547-963e-6c47ec33278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.rllib\n",
    "\n",
    "## Initialising ray (starts background process for distributed computing)\n",
    "ray.shutdown()\n",
    "ray.init(logging_level=\"WARNING\")\n",
    "\n",
    "# Adding our environment just create to ray\n",
    "ray.tune.registry.register_env(\"battery_control\", battery_env_creator)\n",
    "\n",
    "# Setting some logging/checkpoint saving paths\n",
    "SAVE_PATH = \"./tmp/ppo/battery-control-demo-2\"\n",
    "CHECK_SAVE_PATH = SAVE_PATH + \"/training_checkpoints\"\n",
    "OUT_SAVE_PATH = SAVE_PATH + \"/outputs\"\n",
    "\n",
    "AGENT_CONFIG = {\n",
    "    \"framework\": \"torch\",\n",
    "    \"env_config\": {},\n",
    "    \"output\": OUT_SAVE_PATH,\n",
    "    \"output_compress_columns\": [],\n",
    "    \"gamma\": 0.9999999, # we set the discount factor very high\n",
    "    \"log_level\": \"WARNING\",\n",
    "    \"lr\": 5e-5,\n",
    "    # Learning rate schedule.\n",
    "    #\"lr_schedule\": [\n",
    "    #        [0,     0.00005],\n",
    "    #        [4000*29, 0.0000005],\n",
    "    #    ]\n",
    "}\n",
    "\n",
    "agent = ray.rllib.agents.ppo.PPOTrainer(env=\"battery_control\", config=AGENT_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e40df0-c38d-478e-8394-5c77121cdd1c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Training\n",
    "We then train the agent for `NUM_ITERATIONS` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa37d6-e2a0-491f-b0a6-c4b2b8b6e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 5\n",
    "iteration_string = (\"Training iteration: {}, \"\n",
    "                    \"Min reward: {:.3f}, Mean reward: {:.3f}, \"\n",
    "                    \"Max reward: {:.3f}.\")\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    iteration_out = agent.train()\n",
    "    print(iteration_string.format(i,\n",
    "                                  iteration_out['episode_reward_min'], \n",
    "                                  iteration_out['episode_reward_mean'],\n",
    "                                  iteration_out['episode_reward_max']))\n",
    "    print(\"Learning rate:\", iteration_out[\"info\"][\"learner\"][\"default_policy\"][\"learner_stats\"][\"cur_lr\"])\n",
    "    file_name = agent.save(CHECK_SAVE_PATH)\n",
    "    \n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89f954-00b0-49f4-a918-30dd8f2196bb",
   "metadata": {},
   "source": [
    "## 4. Visualising Agent during Training\n",
    "\n",
    "Using the `InteractiveEpisodes` widget we can visualise how the agent evolves during training. In the widget below, we can see how the agent acts (without exploration) after each of the training iterations above. In case something does not converge as expected, this can help us identify the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3795245-b70c-4b5d-b2fa-d349ec4764e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import solara.utils.rllib\n",
    "import solara.plot.widgets\n",
    "\n",
    "episodes_data = solara.utils.rllib.run_episodes_from_checkpoints(agent=agent, check_save_path=CHECK_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9d303-1ab6-4e25-a5e6-6b5c0ba73aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "solara.plot.widgets.InteractiveEpisodes(episodes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4262d-6e4b-4340-94d8-6d242dd8c123",
   "metadata": {},
   "source": [
    "## 5. Debugging Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7203b51-55e1-434c-bd96-39c24364c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"ray.rllib\")\n",
    "logger.setLevel(\"DEBUG\")\n",
    "_ = solara.utils.rllib.run_episode(agent)\n",
    "logger.setLevel(\"WARNING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea0679-e966-4390-8a22-1386b390a24f",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aeef15-706f-4ea8-ac7d-5a50c02f3354",
   "metadata": {},
   "source": [
    "### A. Problem Variants\n",
    "\n",
    "Unsurprisingly, the four components of the environment come in all shapes and sizes. The goal of Solar Agent is to make a solution for one specific problem easily transferable to another. However, inevitably Solar Agent will fail to cover all types, but should enable relatively easy implementation of new variants. Below are a number of variants for each component.\n",
    "\n",
    "#### Solar installation\n",
    "- Capacity\n",
    "- Weather, ...\n",
    "\n",
    "#### Battery\n",
    "- Capacity\n",
    "- Chemistry (affects charging properties)\n",
    "- Modelling choices\n",
    "\n",
    "#### Load\n",
    "- Type: data-based/model-based\n",
    "- Data source\n",
    "- Reactive (if it responds to actions by the agent)\n",
    "\n",
    "#### Grid connection\n",
    "- Pricing scheme\n",
    "- Ability to sell\n",
    "- Ability to charge battery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65d3dea-9707-4993-b7db-1f7fe5d5cf32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
