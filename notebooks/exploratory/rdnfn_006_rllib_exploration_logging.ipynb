{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9491a9d-0dc2-4dda-b53c-487cbb2422fc",
   "metadata": {},
   "source": [
    "# Exploring RLlib for solar agent problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c8785-ec66-4157-b706-710e65318c18",
   "metadata": {},
   "source": [
    "This notebook explores the use of RLlib with the solar agent environment. It is partly based on the [cartpole tutorial notebook by anyscale](https://github.com/anyscale/academy/blob/9317775c393aff06cff06ae58c88f85ce201940d/ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfc5bd9-8dc2-45e1-9583-49e6abcfe899",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea3c62-ca1a-46af-b64f-224758e6cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff76ce9-5b08-46fc-acfe-e9cb2c797fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import ray.tune\n",
    "import ray.rllib\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import gif\n",
    "\n",
    "from solara.constants import PROJECT_PATH\n",
    "import solara.envs.components.solar\n",
    "import solara.envs.components.load\n",
    "import solara.envs.components.grid\n",
    "import solara.envs.battery_control\n",
    "import solara.envs.components.battery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb969bd8-6cfd-4e35-839e-a0d553bc8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialising ray (starts background process for distributed computing)\n",
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a73761-1929-40e0-b053-6cb819e47499",
   "metadata": {},
   "source": [
    "## 1. Setting up the solar agent environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871988f-e93f-4d61-83f4-a59c04a84b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make the environment usable with RLlib\n",
    "# we wrap its creation in functon\n",
    "\n",
    "import solara.utils.logging\n",
    "import logging\n",
    "\n",
    "def battery_env_creator(env_config):\n",
    "    pv_data_path = PROJECT_PATH + \"/data/solar_trace_data/PV_5796.txt\"\n",
    "    load_data_path = PROJECT_PATH + \"/data/solar_trace_data/load_5796.txt\"\n",
    "\n",
    "    # Setting up components of environment\n",
    "    battery_model = solara.envs.components.battery.LithiumIonBattery(size=10, \n",
    "                                                                     chemistry=\"NMC\", \n",
    "                                                                     time_step_len=1)\n",
    "    pv_model = solara.envs.components.solar.DataPV(data_path=pv_data_path)\n",
    "    load_model = solara.envs.components.load.DataLoad(data_path=load_data_path)\n",
    "    grid_model = solara.envs.components.grid.PeakGrid(peak_threshold=1.0)\n",
    "\n",
    "    # Fixing load and PV trace to single sample\n",
    "    episode_num = 12\n",
    "    load_model.fix_start(episode_num)\n",
    "    pv_model.fix_start(episode_num)\n",
    "\n",
    "    env = solara.envs.battery_control.BatteryControlEnv(\n",
    "        battery=battery_model,\n",
    "        pv_system = pv_model,\n",
    "        grid = grid_model,\n",
    "        load = load_model,\n",
    "        logging_level = \"RAY\",\n",
    "    )\n",
    "    \n",
    "    return env\n",
    "\n",
    "ray.tune.registry.register_env(\"battery_control\", battery_env_creator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c78ba-c2d6-472c-acb1-d3a582551598",
   "metadata": {},
   "source": [
    "## 2. Setting up the RLlib agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08753fdc-daa8-4172-8120-d396e1a87359",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./tmp/ppo/battery-control-4\"\n",
    "check_save_path = save_path + \"/training_checkpoints\"\n",
    "out_save_path = save_path + \"/outputs\"\n",
    "\n",
    "trainer = ray.rllib.agents.ppo.PPOTrainer(env=\"battery_control\", config={\n",
    "    \"framework\": \"torch\",\n",
    "    \"env_config\": {},\n",
    "    \"output\": out_save_path,\n",
    "    \"output_compress_columns\": [],\n",
    "    \"gamma\": 0.9999999, # we set the discount factor very high\n",
    "    #\"log_level\": \"WARNING\",\n",
    "    \"log_level\": \"DEBUG\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852a208-31eb-4520-880e-c7d7f885151e",
   "metadata": {},
   "source": [
    "## 3. Training agent on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff419414-7d16-4873-9c6b-a0792a9d2a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 40\n",
    "iteration_string = \"Training iteration: {}, Min reward: {:.3f}, Mean reward: {:.3f}, Max reward: {:.3f}.\"\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    iteration_out = trainer.train()\n",
    "    print(iteration_string.format(i,\n",
    "                                  iteration_out['episode_reward_min'], \n",
    "                                  iteration_out['episode_reward_mean'],\n",
    "                                  iteration_out['episode_reward_max']))\n",
    "\n",
    "    file_name = trainer.save(check_save_path)\n",
    "\n",
    "print(\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff3630-f9cc-442f-9783-1e040e08ce85",
   "metadata": {},
   "source": [
    "## 4. Visualising Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5bb2db-03cc-4b00-85cd-1d35be8c067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode(load, pv_gen, energy_cont, actions, \n",
    "                 rewards, iteration, episode_reward,\n",
    "                 ):\n",
    "    \"\"\"Plot a single episode of battery control problem\"\"\"\n",
    "    \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6,4),dpi=200)\n",
    "    \n",
    "    x = np.arange(0,24)\n",
    "    \n",
    "    ax.set_xticks([0,5,10,15,20,23], minor=False)\n",
    "    ax.set_xticks(x, minor=True)\n",
    "    ax.set_xticklabels([0,5,10,15,20,23], minor=False)\n",
    "    ax.yaxis.grid(True, which='major')\n",
    "    ax.xaxis.grid(True, which='major')\n",
    "    ax.xaxis.grid(True, which='minor')\n",
    "    ax.set_prop_cycle('color',[\"blue\",\"green\",\"black\",\"red\",\"purple\"])\n",
    "    \n",
    "    labels = [\"load (kW)\",\"pv_gen (kW)\",\"energy_cont (kWh)\",\"actions\",\"rewards ($)\"]\n",
    "    \n",
    "    for label, values in zip(labels,[load, pv_gen, energy_cont, actions, rewards]):\n",
    "         plt.plot(x, values, label=label, marker='.')\n",
    "\n",
    "    plt.title(\"Iteration {}      (Overall cost {:.3f})\".format (iteration, -episode_reward))\n",
    "    plt.ylabel(\"kW / kWh / other\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.ylim(ymin=-1.2, ymax = 2.5)\n",
    "    \n",
    "    #fig.savefig(\"tmp/figures/rl_problem_1_progress_ppo_iteration{:02.0f}.png\".format(iteration))\n",
    "    \n",
    "\n",
    "def run_episode(agent, env_config={}):\n",
    "    \"\"\"Run a single episode with an RLlib agent.\"\"\"\n",
    "    \n",
    "    env = agent.env_creator(env_config)\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    actions = []\n",
    "    observations = []\n",
    "    observations.append(obs)\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs, explore=False)\n",
    "        #action = np.array([1])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        actions.append(action)\n",
    "        observations.append(obs)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return (observations, actions, rewards)\n",
    "\n",
    "\n",
    "@widgets.interact(iteration=(1,80))\n",
    "@gif.frame\n",
    "def plot_agent(iteration=10):\n",
    "    \"\"\"Plot agent for a certain training iteration\"\"\"\n",
    "    \n",
    "    trainer.restore(check_save_path + '/checkpoint_0000{:02.0f}/checkpoint-{}'.format(iteration,iteration))\n",
    "\n",
    "    observations, actions, rewards = run_episode(agent=trainer)  \n",
    "    episode_reward = sum(rewards)\n",
    "    \n",
    "    observations = np.array(observations)      \n",
    "    load = observations[:24,0]\n",
    "    pv_gen = observations[:24,1]\n",
    "    energy_cont = observations[:24,2]\n",
    "        \n",
    "    plot_episode(load, pv_gen, energy_cont, actions, rewards, iteration, episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaebfff-c9e8-4e3d-a412-c764fd8d1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GIF\n",
    "\n",
    "frames = []\n",
    "for i in range(1,31):\n",
    "    frame = plot_agent(i)\n",
    "    frames.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0954762-9c5b-423e-adb5-cbbd681b9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "gif.save(frames, 'example4.gif', duration=400, unit=\"ms\", between=\"frames\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
